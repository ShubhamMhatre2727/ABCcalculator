{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive_custome.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZDEcihRHrT8Kls87kstBAG-1Npiyqeeh",
      "authorship_tag": "ABX9TyNkns119ZDKnYPYNJBzLa2p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShubhamMhatre2727/ABCcalculator/blob/main/naive_custome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmmFV96A1MZS",
        "outputId": "9c67f3b7-ed32-49f8-abb4-f98fbb6c9a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation score =  [0.74629375 0.73945313 0.7403625  0.74831563 0.74181562]\n",
            "Train accuracy =78.00%\n",
            "Test accuracy =75.02%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as nlp\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "import warnings\n",
        "\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "\n",
        "tweets = pd.read_csv(\"/content/drive/MyDrive/csvData/training.1600000.processed.noemoticon.csv\", encoding='latin', names=['sentiment', 'id','date', 'query', 'user', 'tweet'])\n",
        "tweets['sentiment'] = tweets['sentiment'].replace(4, 1)\n",
        "tweets.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)\n",
        "\n",
        "(tweets.isnull().sum() / len(tweets))*100\n",
        "tweets['tweet'] = tweets['tweet'].astype('str')\n",
        "\n",
        "positives = tweets['sentiment'][tweets.sentiment == 1]\n",
        "negatives = tweets['sentiment'][tweets.sentiment == 0]\n",
        "\n",
        "stopword = set(stopwords.words('english'))\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "def remove_pattern(text,pattern):\n",
        "    \n",
        "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
        "    r = re.findall(pattern,text)\n",
        "    \n",
        "    # re.sub() removes @user from the sentences in the dataset\n",
        "    for i in r:\n",
        "        text = re.sub(i,\"\",text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "tweets['Tidy_Tweets'] = tweets['tweet'].str.replace(\"http\\S+\", '')\n",
        "tweets['Tidy_Tweets'] = np.vectorize(remove_pattern)(tweets['Tidy_Tweets'], \"@[\\w]*\")\n",
        "tweets['Tidy_Tweets'] = tweets['Tidy_Tweets'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "tweets['Tidy_Tweets'] = tweets['Tidy_Tweets'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "\n",
        "tokenized_tweet = tweets['Tidy_Tweets'].apply(lambda x: x.split())\n",
        "\n",
        "tokenized_tweet.to_csv(\"tokenized_tweets.csv\")\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts = cv.fit_transform(tweets['Tidy_Tweets'].values.astype('U'))\n",
        "\n",
        "X=text_counts\n",
        "y=tweets['sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=19)\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from math import *\n",
        "cnb = ComplementNB()\n",
        "cnb.fit(X_train, y_train)\n",
        "cross_cnb = cross_val_score(cnb, X, y,n_jobs = -1)\n",
        "print(\"Cross Validation score = \",cross_cnb)                \n",
        "print (\"Train accuracy ={:.2f}%\".format(cnb.score(X_train,y_train)*100))\n",
        "print (\"Test accuracy ={:.2f}%\".format(cnb.score(X_test,y_test)*100))\n",
        "train_acc_cnb=cnb.score(X_train,y_train)\n",
        "test_acc_cnb=cnb.score(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGkZoQaKUIUG",
        "outputId": "227488ce-bfd5-4559-90fd-b0e4b4aac416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Validation score =  [0.74775625 0.740425   0.74175313 0.75003438 0.74280625]\n",
            "Train accuracy =78.14%\n",
            "Test accuracy =75.13%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"working on it\"\n",
        "\n",
        "review_vector = cv.transform([review]) # vectorizing\n",
        "print(cnb.predict(review_vector))\n",
        "\n",
        "\n",
        "# nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "score = SentimentIntensityAnalyzer().polarity_scores(review)\n",
        "print(score)\n",
        "\n",
        "# positive sentiment : (compound score >= 0.05) \n",
        "# neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n",
        "# negative sentiment : (compound score <= -0.05)\n",
        "\n",
        "# 1 --> Positive\n",
        "# 0 --> Negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8251FA4CUcsV",
        "outputId": "a48e0e57-adb3-4f84-fda4-7dfe8896285e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
          ]
        }
      ]
    }
  ]
}